{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 源码阅读"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心源码: 4大类U-net模型定义+预训练权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "\n",
    "\"\"\"\n",
    "This script has been taken (and modified) from :\n",
    "https://github.com/ternaus/TernausNet\n",
    "\n",
    "@ARTICLE{arXiv:1801.05746,\n",
    "         author = {V. Iglovikov and A. Shvets},\n",
    "          title = {TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation},\n",
    "        journal = {ArXiv e-prints},\n",
    "         eprint = {1801.05746}, \n",
    "           year = 2018\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------- basic building bloc ---------------------------------\n",
    "def conv3x3(in_, out):\n",
    "    return nn.Conv2d(in_, out, 3, padding=1)\n",
    "\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_, out):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NoOperation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ConvRelu(in_channels, middle_channels),\n",
    "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UNet11(nn.Module):\n",
    "    def __init__(self, num_classes=1, num_filters=32, pretrained=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with VGG11\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = models.vgg11(pretrained=pretrained).features # !!!!!!! 获取预训练权重 并赋值给下面几层 !!!!!!!!\n",
    "\n",
    "        self.relu = self.encoder[1]\n",
    "        self.conv1 = self.encoder[0]\n",
    "        self.conv2 = self.encoder[3]\n",
    "        self.conv3s = self.encoder[6]\n",
    "        self.conv3 = self.encoder[8]\n",
    "        self.conv4s = self.encoder[11]\n",
    "        self.conv4 = self.encoder[13]\n",
    "        self.conv5s = self.encoder[16]\n",
    "        self.conv5 = self.encoder[18]\n",
    "\n",
    "        self.center = DecoderBlock(num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec5 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8)\n",
    "        self.dec4 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4)\n",
    "        self.dec3 = DecoderBlock(num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2)\n",
    "        self.dec2 = DecoderBlock(num_filters * (4 + 2), num_filters * 2 * 2, num_filters)\n",
    "        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)\n",
    "\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.relu(self.conv1(x))\n",
    "        conv2 = self.relu(self.conv2(self.pool(conv1)))\n",
    "        conv3s = self.relu(self.conv3s(self.pool(conv2)))\n",
    "        conv3 = self.relu(self.conv3(conv3s))\n",
    "        conv4s = self.relu(self.conv4s(self.pool(conv3)))\n",
    "        conv4 = self.relu(self.conv4(conv4s))\n",
    "        conv5s = self.relu(self.conv5s(self.pool(conv4)))\n",
    "        conv5 = self.relu(self.conv5(conv5s))\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))  # 取浅层特征和上采样特征进行合并\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "        return self.final(dec1)\n",
    "\n",
    "\n",
    "def unet11(pretrained=False, **kwargs):\n",
    "    \"\"\"\n",
    "    pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with VGG11\n",
    "            carvana - all weights are pre-trained on\n",
    "                Kaggle: Carvana dataset https://www.kaggle.com/c/carvana-image-masking-challenge\n",
    "    \"\"\"\n",
    "    model = UNet11(pretrained=pretrained, **kwargs)\n",
    "\n",
    "    if pretrained == 'carvana':\n",
    "        state = torch.load('TernausNet.pt')\n",
    "        model.load_state_dict(state['model'])\n",
    "    return model\n",
    "\n",
    "\n",
    "class DecoderBlockV2(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n",
    "        super(DecoderBlockV2, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        if is_deconv:\n",
    "            \"\"\"\n",
    "                Paramaters for Deconvolution were chosen to avoid artifacts, following\n",
    "                link https://distill.pub/2016/deconv-checkerboard/\n",
    "            \"\"\"\n",
    "\n",
    "            self.block = nn.Sequential(\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n",
    "                                   padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                ConvRelu(middle_channels, out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class AlbuNet(nn.Module):\n",
    "    \"\"\"\n",
    "        UNet (https://arxiv.org/abs/1505.04597) with Resnet34(https://arxiv.org/abs/1512.03385) encoder\n",
    "\n",
    "        Proposed by Alexander Buslaev: https://www.linkedin.com/in/al-buslaev/\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=1, num_filters=32, pretrained=False, is_deconv=False):\n",
    "        \"\"\"\n",
    "        :param num_classes:\n",
    "        :param num_filters:\n",
    "        :param pretrained:\n",
    "            False - no pre-trained network is used\n",
    "            True  - encoder is pre-trained with resnet34\n",
    "        :is_deconv:\n",
    "            False: bilinear interpolation is used in decoder\n",
    "            True: deconvolution is used in decoder\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = torchvision.models.resnet34(pretrained=pretrained)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Sequential(self.encoder.conv1,\n",
    "                                   self.encoder.bn1,\n",
    "                                   self.encoder.relu,\n",
    "                                   self.pool)\n",
    "\n",
    "        self.conv2 = self.encoder.layer1\n",
    "\n",
    "        self.conv3 = self.encoder.layer2\n",
    "\n",
    "        self.conv4 = self.encoder.layer3\n",
    "\n",
    "        self.conv5 = self.encoder.layer4\n",
    "\n",
    "        self.center = DecoderBlockV2(512, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "\n",
    "        self.dec5 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec4 = DecoderBlockV2(256 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec3 = DecoderBlockV2(128 + num_filters * 8, num_filters * 4 * 2, num_filters * 2, is_deconv)\n",
    "        self.dec2 = DecoderBlockV2(64 + num_filters * 2, num_filters * 2 * 2, num_filters * 2 * 2, is_deconv)\n",
    "        self.dec1 = DecoderBlockV2(num_filters * 2 * 2, num_filters * 2 * 2, num_filters, is_deconv)\n",
    "        self.dec0 = ConvRelu(num_filters, num_filters)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv5 = self.conv5(conv4)\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(dec2)\n",
    "        dec0 = self.dec0(dec1)\n",
    "\n",
    "        return self.final(dec0)\n",
    "\n",
    "\n",
    "class UNetVGG16(nn.Module):\n",
    "    \"\"\"PyTorch U-Net model using VGG16 encoder.\n",
    "\n",
    "    UNet: https://arxiv.org/abs/1505.04597\n",
    "    VGG: https://arxiv.org/abs/1409.1556\n",
    "    Proposed by Vladimir Iglovikov and Alexey Shvets: https://github.com/ternaus/TernausNet\n",
    "\n",
    "    Args:\n",
    "            num_classes (int): Number of output classes.\n",
    "            num_filters (int, optional): Number of filters in the last layer of decoder. Defaults to 32.\n",
    "            dropout_2d (float, optional): Probability factor of dropout layer before output layer. Defaults to 0.2.\n",
    "            pretrained (bool, optional):\n",
    "                False - no pre-trained weights are being used.\n",
    "                True  - VGG encoder is pre-trained on ImageNet.\n",
    "                Defaults to False.\n",
    "            is_deconv (bool, optional):\n",
    "                False: bilinear interpolation is used in decoder.\n",
    "                True: deconvolution is used in decoder.\n",
    "                Defaults to False.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=1, num_filters=32, dropout_2d=0.2, pretrained=False, is_deconv=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_2d = dropout_2d\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.encoder = torchvision.models.vgg16(pretrained=pretrained).features\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Sequential(self.encoder[0],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[2],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv2 = nn.Sequential(self.encoder[5],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[7],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv3 = nn.Sequential(self.encoder[10],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[12],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[14],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv4 = nn.Sequential(self.encoder[17],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[19],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[21],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.conv5 = nn.Sequential(self.encoder[24],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[26],\n",
    "                                   self.relu,\n",
    "                                   self.encoder[28],\n",
    "                                   self.relu)\n",
    "\n",
    "        self.center = DecoderBlockV2(512, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "\n",
    "        self.dec5 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec4 = DecoderBlockV2(512 + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec3 = DecoderBlockV2(256 + num_filters * 8, num_filters * 4 * 2, num_filters * 2, is_deconv)\n",
    "        self.dec2 = DecoderBlockV2(128 + num_filters * 2, num_filters * 2 * 2, num_filters, is_deconv)\n",
    "        self.dec1 = ConvRelu(64 + num_filters, num_filters)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(self.pool(conv1))\n",
    "        conv3 = self.conv3(self.pool(conv2))\n",
    "        conv4 = self.conv4(self.pool(conv3))\n",
    "        conv5 = self.conv5(self.pool(conv4))\n",
    "\n",
    "        center = self.center(self.pool(conv5))\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n",
    "\n",
    "        return self.final(F.dropout2d(dec1, p=self.dropout_2d))\n",
    "\n",
    "\n",
    "class UNetResNet(nn.Module):\n",
    "    \"\"\"PyTorch U-Net model using ResNet(34, 101 or 152) encoder.\n",
    "\n",
    "    UNet: https://arxiv.org/abs/1505.04597\n",
    "    ResNet: https://arxiv.org/abs/1512.03385\n",
    "    Proposed by Alexander Buslaev: https://www.linkedin.com/in/al-buslaev/\n",
    "\n",
    "    Args:\n",
    "            encoder_depth (int): Depth of a ResNet encoder (34, 101 or 152).\n",
    "            num_classes (int): Number of output classes.\n",
    "            num_filters (int, optional): Number of filters in the last layer of decoder. Defaults to 32.\n",
    "            dropout_2d (float, optional): Probability factor of dropout layer before output layer. Defaults to 0.2.\n",
    "            pretrained (bool, optional):\n",
    "                False - no pre-trained weights are being used.\n",
    "                True  - ResNet encoder is pre-trained on ImageNet.\n",
    "                Defaults to False.\n",
    "            is_deconv (bool, optional):\n",
    "                False: bilinear interpolation is used in decoder.\n",
    "                True: deconvolution is used in decoder.\n",
    "                Defaults to False.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_depth, num_classes, num_filters=32, dropout_2d=0.2,\n",
    "                 pretrained=False, is_deconv=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_2d = dropout_2d\n",
    "\n",
    "        if encoder_depth == 34:\n",
    "            self.encoder = torchvision.models.resnet34(pretrained=pretrained)\n",
    "            bottom_channel_nr = 512\n",
    "        elif encoder_depth == 101:\n",
    "            self.encoder = torchvision.models.resnet101(pretrained=pretrained)\n",
    "            bottom_channel_nr = 2048\n",
    "        elif encoder_depth == 152:\n",
    "            self.encoder = torchvision.models.resnet152(pretrained=pretrained)\n",
    "            bottom_channel_nr = 2048\n",
    "        else:\n",
    "            raise NotImplementedError('only 34, 101, 152 version of Resnet are implemented')\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv1 = nn.Sequential(self.encoder.conv1,\n",
    "                                   self.encoder.bn1,\n",
    "                                   self.encoder.relu,\n",
    "                                   self.pool)\n",
    "\n",
    "        self.conv2 = self.encoder.layer1\n",
    "\n",
    "        self.conv3 = self.encoder.layer2\n",
    "\n",
    "        self.conv4 = self.encoder.layer3\n",
    "\n",
    "        self.conv5 = self.encoder.layer4\n",
    "\n",
    "        self.center = DecoderBlockV2(bottom_channel_nr, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec5 = DecoderBlockV2(bottom_channel_nr + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n",
    "        self.dec4 = DecoderBlockV2(bottom_channel_nr // 2 + num_filters * 8, num_filters * 8 * 2, num_filters * 8,\n",
    "                                   is_deconv)\n",
    "        self.dec3 = DecoderBlockV2(bottom_channel_nr // 4 + num_filters * 8, num_filters * 4 * 2, num_filters * 2,\n",
    "                                   is_deconv)\n",
    "        self.dec2 = DecoderBlockV2(bottom_channel_nr // 8 + num_filters * 2, num_filters * 2 * 2, num_filters * 2 * 2,\n",
    "                                   is_deconv)\n",
    "        self.dec1 = DecoderBlockV2(num_filters * 2 * 2, num_filters * 2 * 2, num_filters, is_deconv)\n",
    "        self.dec0 = ConvRelu(num_filters, num_filters)\n",
    "        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv5 = self.conv5(conv4)\n",
    "\n",
    "        pool = self.pool(conv5)\n",
    "        center = self.center(pool)\n",
    "\n",
    "        dec5 = self.dec5(torch.cat([center, conv5], 1))\n",
    "\n",
    "        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n",
    "        dec1 = self.dec1(dec2)\n",
    "        dec0 = self.dec0(dec1)\n",
    "\n",
    "        return self.final(F.dropout2d(dec0, p=self.dropout_2d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1：处理原始数据并保存（只运行1次）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------ 处理原始数据并保存 ----------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "class ArgParser(object):\n",
    "    def __init__(self):\n",
    "        self.train_images_dir=\"TrainData\"\n",
    "        self.test_images_dir=\"TestData\"\n",
    "        self.depths_filepath=\"depths.csv\"\n",
    "        self.metadata_filepath=\"Results/metadata.csv\"\n",
    "        \n",
    "PARAMS = ArgParser()\n",
    "\n",
    "def generate_metadata(train_images_dir, test_images_dir, depths_filepath):\n",
    "    depths = pd.read_csv(depths_filepath)\n",
    "\n",
    "    metadata = {}\n",
    "    print(os.path.join(train_images_dir, 'images'))\n",
    "    \n",
    "    for filename in (os.listdir(os.path.join(train_images_dir, 'images'))):\n",
    "        image_filepath = os.path.join(train_images_dir, 'images', filename)\n",
    "        mask_filepath = os.path.join(train_images_dir, 'masks', filename)\n",
    "        image_id = filename.split('.')[0]\n",
    "        depth = depths[depths['id'] == image_id]['z'].values[0]\n",
    "\n",
    "        metadata.setdefault('file_path_image', []).append(image_filepath)\n",
    "        metadata.setdefault('file_path_mask', []).append(mask_filepath)\n",
    "        metadata.setdefault('is_train', []).append(1)\n",
    "        metadata.setdefault('id', []).append(image_id)\n",
    "        metadata.setdefault('z', []).append(depth)\n",
    "\n",
    "    for filename in (os.listdir(os.path.join(test_images_dir, 'images'))):\n",
    "        image_filepath = os.path.join(test_images_dir, 'images', filename)\n",
    "        image_id = filename.split('.')[0]\n",
    "        depth = depths[depths['id'] == image_id]['z'].values[0]\n",
    "\n",
    "        metadata.setdefault('file_path_image', []).append(image_filepath)\n",
    "        metadata.setdefault('file_path_mask', []).append(None)\n",
    "        metadata.setdefault('is_train', []).append(0)\n",
    "        metadata.setdefault('id', []).append(image_id)\n",
    "        metadata.setdefault('z', []).append(depth)\n",
    "    \n",
    "    print(\"here .... \")\n",
    "    return pd.DataFrame(metadata)  # 直接将DataFrame中的数据写到csv文件中\n",
    "\n",
    "def prepare_metadata():\n",
    "    #LOGGER.info('creating metadata')\n",
    "    meta = generate_metadata(train_images_dir=PARAMS.train_images_dir,   # 训练数据存放路径\n",
    "                             test_images_dir=PARAMS.test_images_dir,     # 测试数据存放路径\n",
    "                             depths_filepath=PARAMS.depths_filepath      # 深度数据存放路径\n",
    "                             )\n",
    "    meta.to_csv(PARAMS.metadata_filepath, index=None)\n",
    "\n",
    "# 只运行一次就ok\n",
    "# prepare_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2：训练数据阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "attrdict.dictionary.AttrDict"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from attrdict import AttrDict\n",
    "from src.augmentation import intensity_seq, affine_seq, pad_to_fit_net, crop_seq, test_time_augmentation_transform, \\\n",
    "    test_time_augmentation_inverse_transform\n",
    "\n",
    "class ArgParser(object):\n",
    "    def __init__(self):\n",
    "        # PreProcess\n",
    "        self.train_images_dir=\"trainData\"\n",
    "        self.test_images_dir=\"testData\"\n",
    "        self.depths_filepath=\"depths.csv\"\n",
    "        self.metadata_filepath=\"Results/metadata.csv\"\n",
    "        \n",
    "        # Training \n",
    "        self.experiment_dir=\"Results\" \n",
    "        self.clone_experiment_dir_from='' \n",
    "        self.overwrite=1\n",
    "        self.SEED = 1234\n",
    "        self.shuffle=1\n",
    "        self.n_cv_splits=10 # 10折交叉验证\n",
    "        self.dev_mode_size=20\n",
    "        self.num_workers=2\n",
    "        self.num_threads=4\n",
    "        \n",
    "        # General parameters\n",
    "        self.image_h = 128\n",
    "        self.image_w = 128\n",
    "        self.image_channels = 3\n",
    "        \n",
    "        self.epochs_nr=10000\n",
    "        self.batch_size_train=64\n",
    "        self.batch_size_inference=64\n",
    "        self.lr=0.0001\n",
    "        self.momentum=0.9\n",
    "        self.gamma=0.95\n",
    "        self.patience= 20\n",
    "        self.validation_metric_name='iout'\n",
    "        self.minimize_validation_metric= 0\n",
    "        \n",
    "        self.loader_mode=\"resize\"\n",
    "        self.pad_method=\"symmetric\"\n",
    "        \n",
    "        self.image_source=\"disk\"\n",
    "        self.target_format = 'png'\n",
    "        self.pin_memory = 1\n",
    "        \n",
    "        # U-Net parameters\n",
    "        self.unet_output_channels=2  # --------------- 确实是2通道 --------------\n",
    "        self.unet_activation='sigmoid'\n",
    "        self.encoder=\"ResNet152\"\n",
    "\n",
    "        # U-Net from scratch parameters\n",
    "        self.nr_unet_outputs=1\n",
    "        self.n_filters=16\n",
    "        self.conv_kernel= 3\n",
    "        self.pool_kernel= 3\n",
    "        self.pool_stride= 2\n",
    "        self.repeat_blocks= 4\n",
    "\n",
    "        # Loss\n",
    "        self.dice_weight=5.0\n",
    "        self.bce_weight=1.0\n",
    "        \n",
    "        # Regularization\n",
    "        self.use_batch_norm=1\n",
    "        self.l2_reg_conv=0.0001\n",
    "        self.l2_reg_dense=0.0\n",
    "        self.dropout_conv= 0.1\n",
    "        self.dropout_dense= 0.0\n",
    "\n",
    "        # Postprocessing\n",
    "        self.threshold_masks= 0.5\n",
    "        self.tta_aggregation_method= \"mean\"\n",
    "\n",
    "        \n",
    "PARAMS = ArgParser()\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "ID_COLUMNS = ['id']\n",
    "X_COLUMNS = ['file_path_image']\n",
    "Y_COLUMNS = ['file_path_mask']\n",
    "DEPTH_COLUMN = ['z']\n",
    "ORIGINAL_SIZE = (101, 101)\n",
    "\n",
    "GLOBAL_CONFIG = {'exp_root': PARAMS.experiment_dir,\n",
    "                 'num_workers': PARAMS.num_workers,\n",
    "                 'num_classes': 2,\n",
    "                 'img_H-W': (PARAMS.image_h, PARAMS.image_w),\n",
    "                 'batch_size_train': PARAMS.batch_size_train,\n",
    "                 'batch_size_inference': PARAMS.batch_size_inference,\n",
    "                 'loader_mode': PARAMS.loader_mode,\n",
    "                 }\n",
    "\n",
    "TRAINING_CONFIG = {'epochs': PARAMS.epochs_nr,\n",
    "                   'shuffle': True,\n",
    "                   'batch_size': PARAMS.batch_size_train,\n",
    "                   }\n",
    "\n",
    "SOLUTION_CONFIG = AttrDict({\n",
    "    'env': {'experiment_dir': PARAMS.experiment_dir},\n",
    "    'execution': GLOBAL_CONFIG,\n",
    "    'xy_splitter': {\n",
    "        'unet': {'x_columns': X_COLUMNS,\n",
    "                 'y_columns': Y_COLUMNS,\n",
    "                 },\n",
    "    },\n",
    "    'reader': {\n",
    "        'unet': {'x_columns': X_COLUMNS,\n",
    "                 'y_columns': Y_COLUMNS,\n",
    "                 },\n",
    "    },\n",
    "    'loaders': {'crop_and_pad': {'dataset_params': {'h': PARAMS.image_h,\n",
    "                                                    'w': PARAMS.image_w,\n",
    "                                                    'pad_method': PARAMS.pad_method,\n",
    "                                                    'image_source': PARAMS.image_source,\n",
    "                                                    'divisor': 64,\n",
    "                                                    'target_format': PARAMS.target_format,\n",
    "                                                    'MEAN': MEAN,\n",
    "                                                    'STD': STD\n",
    "                                                    },\n",
    "                                 'loader_params': {'training': {'batch_size': PARAMS.batch_size_train,\n",
    "                                                                'shuffle': True,\n",
    "                                                                'num_workers': PARAMS.num_workers,\n",
    "                                                                'pin_memory': PARAMS.pin_memory\n",
    "                                                                },\n",
    "                                                   'inference': {'batch_size': PARAMS.batch_size_inference,\n",
    "                                                                 'shuffle': False,\n",
    "                                                                 'num_workers': PARAMS.num_workers,\n",
    "                                                                 'pin_memory': PARAMS.pin_memory\n",
    "                                                                 },\n",
    "                                                   },\n",
    "\n",
    "                                 'augmentation_params': {'image_augment_train': intensity_seq,\n",
    "                                                         'image_augment_with_target_train': crop_seq(\n",
    "                                                             crop_size=(PARAMS.image_h, PARAMS.image_w)),\n",
    "                                                         'image_augment_inference': pad_to_fit_net(64,\n",
    "                                                                                                   PARAMS.pad_method),\n",
    "                                                         'image_augment_with_target_inference': pad_to_fit_net(64,\n",
    "                                                                                                               PARAMS.pad_method)\n",
    "                                                         },\n",
    "                                 },\n",
    "                'crop_and_pad_tta': {'dataset_params': {'h': PARAMS.image_h,\n",
    "                                                        'w': PARAMS.image_w,\n",
    "                                                        'pad_method': PARAMS.pad_method,\n",
    "                                                        'image_source': PARAMS.image_source,\n",
    "                                                        'divisor': 64,\n",
    "                                                        'target_format': PARAMS.target_format,\n",
    "                                                        'MEAN': MEAN,\n",
    "                                                        'STD': STD\n",
    "                                                        },\n",
    "                                     'loader_params': {'training': {'batch_size': PARAMS.batch_size_train,\n",
    "                                                                    'shuffle': True,\n",
    "                                                                    'num_workers': PARAMS.num_workers,\n",
    "                                                                    'pin_memory': PARAMS.pin_memory\n",
    "                                                                    },\n",
    "                                                       'inference': {'batch_size': PARAMS.batch_size_inference,\n",
    "                                                                     'shuffle': False,\n",
    "                                                                     'num_workers': PARAMS.num_workers,\n",
    "                                                                     'pin_memory': PARAMS.pin_memory\n",
    "                                                                     },\n",
    "                                                       },\n",
    "\n",
    "                                     'augmentation_params': {\n",
    "                                         'image_augment_inference': pad_to_fit_net(64, PARAMS.pad_method),\n",
    "                                         'image_augment_with_target_inference': pad_to_fit_net(64,\n",
    "                                                                                               PARAMS.pad_method),\n",
    "                                         'tta_transform': test_time_augmentation_transform\n",
    "                                     },\n",
    "                                     },\n",
    "                'resize': {'dataset_params': {'h': PARAMS.image_h,\n",
    "                                              'w': PARAMS.image_w,\n",
    "                                              'pad_method': PARAMS.pad_method,\n",
    "                                              'image_source': PARAMS.image_source,\n",
    "                                              'divisor': 64,\n",
    "                                              'target_format': PARAMS.target_format,\n",
    "                                              'MEAN': MEAN,\n",
    "                                              'STD': STD\n",
    "                                              },\n",
    "                           'loader_params': {'training': {'batch_size': PARAMS.batch_size_train,\n",
    "                                                          'shuffle': True,\n",
    "                                                          'num_workers': PARAMS.num_workers,\n",
    "                                                          'pin_memory': PARAMS.pin_memory\n",
    "                                                          },\n",
    "                                             'inference': {'batch_size': PARAMS.batch_size_inference,\n",
    "                                                           'shuffle': False,\n",
    "                                                           'num_workers': PARAMS.num_workers,\n",
    "                                                           'pin_memory': PARAMS.pin_memory\n",
    "                                                           },\n",
    "                                             },\n",
    "\n",
    "                           'augmentation_params': {'image_augment_train': intensity_seq,\n",
    "                                                   'image_augment_with_target_train': affine_seq\n",
    "                                                   },\n",
    "                           },\n",
    "                'resize_tta': {'dataset_params': {'h': PARAMS.image_h,\n",
    "                                                  'w': PARAMS.image_w,\n",
    "                                                  'pad_method': PARAMS.pad_method,\n",
    "                                                  'image_source': PARAMS.image_source,\n",
    "                                                  'divisor': 64,\n",
    "                                                  'target_format': PARAMS.target_format,\n",
    "                                                  'MEAN': MEAN,\n",
    "                                                  'STD': STD\n",
    "                                                  },\n",
    "                               'loader_params': {'training': {'batch_size': PARAMS.batch_size_train,\n",
    "                                                              'shuffle': True,\n",
    "                                                              'num_workers': PARAMS.num_workers,\n",
    "                                                              'pin_memory': PARAMS.pin_memory\n",
    "                                                              },\n",
    "                                                 'inference': {'batch_size': PARAMS.batch_size_inference,\n",
    "                                                               'shuffle': False,\n",
    "                                                               'num_workers': PARAMS.num_workers,\n",
    "                                                               'pin_memory': PARAMS.pin_memory\n",
    "                                                               },\n",
    "                                                 },\n",
    "\n",
    "                               'augmentation_params': {'tta_transform': test_time_augmentation_transform\n",
    "                                                       },\n",
    "                               },\n",
    "                },\n",
    "    'model': {\n",
    "        'unet': {\n",
    "            'architecture_config': {'model_params': {'n_filters': PARAMS.n_filters,\n",
    "                                                     'conv_kernel': PARAMS.conv_kernel,\n",
    "                                                     'pool_kernel': PARAMS.pool_kernel,\n",
    "                                                     'pool_stride': PARAMS.pool_stride,\n",
    "                                                     'repeat_blocks': PARAMS.repeat_blocks,\n",
    "                                                     'batch_norm': PARAMS.use_batch_norm,\n",
    "                                                     'dropout': PARAMS.dropout_conv,\n",
    "                                                     'in_channels': PARAMS.image_channels,\n",
    "                                                     'out_channels': PARAMS.unet_output_channels,\n",
    "                                                     'nr_outputs': PARAMS.nr_unet_outputs,\n",
    "                                                     'encoder': PARAMS.encoder,\n",
    "                                                     'activation': PARAMS.unet_activation,\n",
    "                                                     'dice_weight': PARAMS.dice_weight,\n",
    "                                                     'bce_weight': PARAMS.bce_weight,\n",
    "                                                     },\n",
    "                                    'optimizer_params': {'lr': PARAMS.lr,\n",
    "                                                         },\n",
    "                                    'regularizer_params': {'regularize': True,\n",
    "                                                           'weight_decay_conv2d': PARAMS.l2_reg_conv,\n",
    "                                                           },\n",
    "                                    'weights_init': {'function': 'xavier',\n",
    "                                                     },\n",
    "                                    },\n",
    "            'training_config': TRAINING_CONFIG,\n",
    "            'callbacks_config': {'model_checkpoint': {\n",
    "                'filepath': os.path.join(GLOBAL_CONFIG['exp_root'], 'checkpoints', 'unet', 'best.torch'),\n",
    "                'epoch_every': 1,\n",
    "                'metric_name': PARAMS.validation_metric_name,\n",
    "                'minimize': PARAMS.minimize_validation_metric},\n",
    "                'lr_scheduler': {'gamma': PARAMS.gamma,\n",
    "                                 'epoch_every': 1},\n",
    "                'training_monitor': {'batch_every': 0,\n",
    "                                     'epoch_every': 1},\n",
    "                'experiment_timing': {'batch_every': 0,\n",
    "                                      'epoch_every': 1},\n",
    "                'validation_monitor': {'epoch_every': 1,\n",
    "                                       'data_dir': PARAMS.train_images_dir,\n",
    "                                       'loader_mode': PARAMS.loader_mode},\n",
    "                'neptune_monitor': {'model_name': 'unet',\n",
    "                                    'image_nr': 4,\n",
    "                                    'image_resize': 0.2},\n",
    "                'early_stopping': {'patience': PARAMS.patience,\n",
    "                                   'metric_name': PARAMS.validation_metric_name,\n",
    "                                   'minimize': PARAMS.minimize_validation_metric},\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    'tta_generator': {'flip_ud': False,\n",
    "                      'flip_lr': True,\n",
    "                      'rotation': False,\n",
    "                      'color_shift_runs': 4},\n",
    "    'tta_aggregator': {'tta_inverse_transform': test_time_augmentation_inverse_transform,\n",
    "                       'method': PARAMS.tta_aggregation_method,\n",
    "                       'nthreads': PARAMS.num_threads\n",
    "                       },\n",
    "    'thresholder': {'threshold_masks': PARAMS.threshold_masks,\n",
    "                    },\n",
    "})\n",
    "\n",
    "print(\"Done!\")\n",
    "type(SOLUTION_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PRETRAINED_NETWORKS = {'VGG11': {'model': UNet11,\n",
    "                                 'model_config': {'pretrained': True},\n",
    "                                 'init_weights': False},\n",
    "                       'VGG16': {'model': UNetVGG16,\n",
    "                                 'model_config': {'pretrained': True,\n",
    "                                                  'dropout_2d': 0.0, 'is_deconv': True},\n",
    "                                 'init_weights': False},\n",
    "                       'AlbuNet': {'model': AlbuNet,\n",
    "                                   'model_config': {'pretrained': True, 'is_deconv': True},\n",
    "                                   'init_weights': False},\n",
    "                       'ResNet34': {'model': UNetResNet,\n",
    "                                    'model_config': {'encoder_depth': 34,\n",
    "                                                     'num_filters': 32, 'dropout_2d': 0.0,\n",
    "                                                     'pretrained': True, 'is_deconv': True,\n",
    "                                                     },\n",
    "                                    'init_weights': False},\n",
    "                       'ResNet101': {'model': UNetResNet,\n",
    "                                     'model_config': {'encoder_depth': 101,\n",
    "                                                      'num_filters': 32, 'dropout_2d': 0.0,\n",
    "                                                      'pretrained': True, 'is_deconv': True,\n",
    "                                                      },\n",
    "                                     'init_weights': False},\n",
    "                       'ResNet152': {'model': UNetResNet,\n",
    "                                     'model_config': {'encoder_depth': 152,\n",
    "                                                      'num_filters': 32, 'dropout_2d': 0.2,\n",
    "                                                      'pretrained': True, 'is_deconv': True,\n",
    "                                                      },\n",
    "                                     'init_weights': False}\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '__main__.pipelines'; '__main__' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0310f63266cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#from .metrics import intersection_over_union, intersection_over_union_thresholds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#from . import pipeline_config as cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPIPELINES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeptuneContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_submission\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0mgenerate_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKFoldBySortedValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '__main__.pipelines'; '__main__' is not a package"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "#    src/pipeline_manager.py        #\n",
    "#####################################\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from .metrics import intersection_over_union, intersection_over_union_thresholds\n",
    "from . import pipeline_config as cfg\n",
    "\n",
    "from .utils import NeptuneContext, init_logger, read_masks, create_submission, \\\n",
    "    generate_metadata, set_seed, KFoldBySortedValue, clean_memory\n",
    "\n",
    "class ArgParser(object):\n",
    "    def __init__(self):\n",
    "        # PreProcess\n",
    "        self.train_images_dir=\"trainData\"\n",
    "        self.test_images_dir=\"testData\"\n",
    "        self.depths_filepath=\"depths.csv\"\n",
    "        self.metadata_filepath=\"Results/metadata.csv\"\n",
    "        \n",
    "        # Training \n",
    "        self.experiment_dir=\"Results\" \n",
    "        self.clone_experiment_dir_from='' \n",
    "        self.overwrite=1\n",
    "        self.SEED = 1234\n",
    "        self.shuffle=1\n",
    "        self.n_cv_splits=10 # 10折交叉验证\n",
    "        self.dev_mode_size=20\n",
    "        \n",
    "        \n",
    "LOGGER = init_logger()\n",
    "CTX = NeptuneContext()\n",
    "PARAMS = ArgParser()\n",
    "set_seed(cfg.SEED)\n",
    "\n",
    "\n",
    "class PipelineManager:\n",
    "    def prepare_metadata(self):\n",
    "        prepare_metadata()\n",
    "\n",
    "    def train(self, pipeline_name, dev_mode):\n",
    "        train(pipeline_name, dev_mode)\n",
    "\n",
    "    def evaluate(self, pipeline_name, dev_mode):\n",
    "        evaluate(pipeline_name, dev_mode)\n",
    "\n",
    "    def predict(self, pipeline_name, dev_mode, submit_predictions):\n",
    "        predict(pipeline_name, dev_mode, submit_predictions)\n",
    "\n",
    "    def train_evaluate_cv(self, pipeline_name, dev_mode):\n",
    "        train_evaluate_cv(pipeline_name, dev_mode)\n",
    "\n",
    "\n",
    "def train(pipeline_name, dev_mode):\n",
    "    #LOGGER.info('training')\n",
    "    print(\"training ...\")\n",
    "    \n",
    "    if PARAMS.clone_experiment_dir_from != '':\n",
    "        if os.path.exists(PARAMS.experiment_dir):\n",
    "            shutil.rmtree(PARAMS.experiment_dir)  # 递归删除该目录\n",
    "        shutil.copytree(PARAMS.clone_experiment_dir_from, PARAMS.experiment_dir)  # 拷贝文件 source -->target\n",
    "    \n",
    "    # 判断是否覆盖结果\n",
    "    if bool(PARAMS.overwrite) and os.path.isdir(PARAMS.experiment_dir):\n",
    "        shutil.rmtree(PARAMS.experiment_dir)\n",
    "    \n",
    "    # 读入metadata\n",
    "    meta = pd.read_csv(PARAMS.metadata_filepath) \n",
    "    # 索引所有训练数据\n",
    "    meta_train = meta[meta['is_train'] == 1]\n",
    "\n",
    "    cv = KFoldBySortedValue(n_splits=PARAMS.n_cv_splits, shuffle=PARAMS.shuffle, random_state=PARAMS.SEED)\n",
    "    \n",
    "    for train_idx, valid_idx in cv.split(meta_train[['z']].values.reshape(-1)):\n",
    "        break\n",
    "\n",
    "    meta_train_split, meta_valid_split = meta_train.iloc[train_idx], meta_train.iloc[valid_idx]\n",
    "\n",
    "    if dev_mode:\n",
    "        meta_train_split = meta_train_split.sample(PARAMS.dev_mode_size, random_state=PARAMS.SEED)\n",
    "        meta_valid_split = meta_valid_split.sample(int(PARAMS.dev_mode_size / 2), random_state=PARAMS.SEED)\n",
    "\n",
    "    \n",
    "    data = {'input': {'meta': meta_train_split},\n",
    "            'callback_input': {'meta_valid': meta_valid_split} \n",
    "           }\n",
    "    PIPELINES = {'unet': unet, 'unet_tta': unet_tta } # 先确实选择好选用哪种模型\n",
    "    \n",
    "    \n",
    "    pipeline = PIPELINES[pipeline_name](config=cfg.SOLUTION_CONFIG, train_mode=True)\n",
    "    pipeline.clean_cache()\n",
    "    pipeline.fit_transform(data)\n",
    "    pipeline.clean_cache()\n",
    "\n",
    "\n",
    "def evaluate(pipeline_name, dev_mode):\n",
    "    LOGGER.info('evaluating')\n",
    "\n",
    "    if PARAMS.clone_experiment_dir_from != '':\n",
    "        if os.path.exists(PARAMS.experiment_dir):\n",
    "            shutil.rmtree(PARAMS.experiment_dir)\n",
    "        shutil.copytree(PARAMS.clone_experiment_dir_from, PARAMS.experiment_dir)\n",
    "\n",
    "    meta = pd.read_csv(PARAMS.metadata_filepath)\n",
    "    meta_train = meta[meta['is_train'] == 1]\n",
    "\n",
    "    cv = KFoldBySortedValue(n_splits=PARAMS.n_cv_splits, shuffle=PARAMS.shuffle, random_state=cfg.SEED)\n",
    "    for train_idx, valid_idx in cv.split(meta_train[cfg.DEPTH_COLUMN].values.reshape(-1)):\n",
    "        break\n",
    "\n",
    "    meta_valid_split = meta_train.iloc[valid_idx]\n",
    "    y_true = read_masks(meta_valid_split[cfg.Y_COLUMNS[0]].values)\n",
    "\n",
    "    if dev_mode:\n",
    "        meta_valid_split = meta_valid_split.sample(PARAMS.dev_mode_size, random_state=cfg.SEED)\n",
    "\n",
    "    \n",
    "    data = {'input': {'meta': meta_valid_split,},\n",
    "            \n",
    "            'callback_input': {'meta_valid': None}\n",
    "            }\n",
    "    \n",
    "    \n",
    "    \n",
    "    pipeline = PIPELINES[pipeline_name](config=cfg.SOLUTION_CONFIG, train_mode=False)\n",
    "    pipeline.clean_cache()\n",
    "    output = pipeline.transform(data)\n",
    "    pipeline.clean_cache()\n",
    "    y_pred = output['y_pred']\n",
    "\n",
    "    LOGGER.info('Calculating IOU and IOUT Scores')\n",
    "    iou_score = intersection_over_union(y_true, y_pred)\n",
    "    LOGGER.info('IOU score on validation is {}'.format(iou_score))\n",
    "    CTX.channel_send('IOU', 0, iou_score)\n",
    "\n",
    "    iout_score = intersection_over_union_thresholds(y_true, y_pred)\n",
    "    LOGGER.info('IOUT score on validation is {}'.format(iout_score))\n",
    "    CTX.channel_send('IOUT', 0, iout_score)\n",
    "\n",
    "    results_filepath = os.path.join(PARAMS.experiment_dir, 'validation_results.pkl')\n",
    "    LOGGER.info('Saving validation results to {}'.format(results_filepath))\n",
    "    joblib.dump((meta_valid_split, y_true, y_pred), results_filepath)\n",
    "\n",
    "\n",
    "def make_submission(submission_filepath):\n",
    "    LOGGER.info('Making Kaggle submit...')\n",
    "    os.system('kaggle competitions submit -c tgs-salt-identification-challenge -f {} -m {}'.format(submission_filepath,\n",
    "                                                                                                   PARAMS.kaggle_message))\n",
    "    LOGGER.info('Kaggle submit completed')\n",
    "\n",
    "\n",
    "def predict(pipeline_name, submit_predictions, dev_mode):\n",
    "    LOGGER.info('predicting')\n",
    "\n",
    "    if PARAMS.clone_experiment_dir_from != '':\n",
    "        if os.path.exists(PARAMS.experiment_dir):\n",
    "            shutil.rmtree(PARAMS.experiment_dir)\n",
    "        shutil.copytree(PARAMS.clone_experiment_dir_from, PARAMS.experiment_dir)\n",
    "\n",
    "    meta = pd.read_csv(PARAMS.metadata_filepath)\n",
    "    meta_test = meta[meta['is_train'] == 0]\n",
    "\n",
    "    if dev_mode:\n",
    "        meta_test = meta_test.sample(PARAMS.dev_mode_size, random_state=cfg.SEED)\n",
    "\n",
    "    data = {'input': {'meta': meta_test,\n",
    "                      },\n",
    "            'callback_input': {'meta_valid': None\n",
    "                               }\n",
    "            }\n",
    "\n",
    "    pipeline = PIPELINES[pipeline_name](config=cfg.SOLUTION_CONFIG, train_mode=False)\n",
    "    pipeline.clean_cache()\n",
    "    output = pipeline.transform(data)\n",
    "    pipeline.clean_cache()\n",
    "    y_pred = output['y_pred']\n",
    "\n",
    "    submission = create_submission(meta_test, y_pred)\n",
    "\n",
    "    submission_filepath = os.path.join(PARAMS.experiment_dir, 'submission.csv')\n",
    "\n",
    "    submission.to_csv(submission_filepath, index=None, encoding='utf-8')\n",
    "    LOGGER.info('submission saved to {}'.format(submission_filepath))\n",
    "    LOGGER.info('submission head \\n\\n{}'.format(submission.head()))\n",
    "\n",
    "    if submit_predictions:\n",
    "        make_submission(submission_filepath)\n",
    "\n",
    "\n",
    "def train_evaluate_cv(pipeline_name, dev_mode):\n",
    "    LOGGER.info('training')\n",
    "\n",
    "    if PARAMS.clone_experiment_dir_from != '':\n",
    "        if os.path.exists(PARAMS.experiment_dir):\n",
    "            shutil.rmtree(PARAMS.experiment_dir)\n",
    "        shutil.copytree(PARAMS.clone_experiment_dir_from, PARAMS.experiment_dir)\n",
    "\n",
    "    if bool(PARAMS.overwrite) and os.path.isdir(PARAMS.experiment_dir):\n",
    "        shutil.rmtree(PARAMS.experiment_dir)\n",
    "\n",
    "    if dev_mode:\n",
    "        meta = pd.read_csv(PARAMS.metadata_filepath, nrows=PARAMS.dev_mode_size)\n",
    "    else:\n",
    "        meta = pd.read_csv(PARAMS.metadata_filepath)\n",
    "    meta_train = meta[meta['is_train'] == 1]\n",
    "\n",
    "    cv = KFoldBySortedValue(n_splits=PARAMS.n_cv_splits, shuffle=PARAMS.shuffle, random_state=cfg.SEED)\n",
    "\n",
    "    fold_iou, fold_iout = [], []\n",
    "    for fold_id, (train_idx, valid_idx) in enumerate(cv.split(meta_train[cfg.DEPTH_COLUMN].values.reshape(-1))):\n",
    "        train_data_split, valid_data_split = meta_train.iloc[train_idx], meta_train.iloc[valid_idx]\n",
    "\n",
    "        LOGGER.info('Started fold {}'.format(fold_id))\n",
    "        iou, iout, _, _ = _fold_fit_evaluate_loop(train_data_split,\n",
    "                                                  valid_data_split,\n",
    "                                                  fold_id,\n",
    "                                                  pipeline_name)\n",
    "\n",
    "        LOGGER.info('Fold {} IOU {}'.format(fold_id, iou))\n",
    "        CTX.channel_send('Fold {} IOU'.format(fold_id), 0, iou)\n",
    "        LOGGER.info('Fold {} IOUT {}'.format(fold_id, iout))\n",
    "        CTX.channel_send('Fold {} IOUT'.format(fold_id), 0, iout)\n",
    "\n",
    "        fold_iou.append(iou)\n",
    "        fold_iout.append(iout)\n",
    "\n",
    "    iou_mean, iou_std = np.mean(fold_iou), np.std(fold_iou)\n",
    "    iout_mean, iout_std = np.mean(fold_iout), np.std(fold_iout)\n",
    "\n",
    "    LOGGER.info('IOU mean {}, IOU std {}'.format(iou_mean, iou_std))\n",
    "    CTX.channel_send('IOU', 0, iou_mean)\n",
    "    CTX.channel_send('IOU STD', 0, iou_std)\n",
    "\n",
    "    LOGGER.info('IOUT mean {}, IOUT std {}'.format(iout_mean, iout_std))\n",
    "    CTX.channel_send('IOUT', 0, iout_mean)\n",
    "    CTX.channel_send('IOUT STD', 0, iout_std)\n",
    "\n",
    "\n",
    "def _fold_fit_evaluate_loop(train_data_split, valid_data_split, fold_id, pipeline_name):\n",
    "    train_pipe_input = {'input': {'meta': train_data_split\n",
    "                                  },\n",
    "                        'callback_input': {'meta_valid': valid_data_split\n",
    "                                           }\n",
    "                        }\n",
    "\n",
    "    valid_pipe_input = {'input': {'meta': valid_data_split\n",
    "                                  },\n",
    "                        'callback_input': {'meta_valid': None\n",
    "                                           }\n",
    "                        }\n",
    "\n",
    "    config = cfg.SOLUTION_CONFIG\n",
    "    model_name = pipeline_name\n",
    "    config['model'][model_name]['callbacks_config']['neptune_monitor']['model_name'] = '{}_{}'.format(model_name,\n",
    "                                                                                                      fold_id)\n",
    "    pipeline = PIPELINES[pipeline_name](config=config, train_mode=True, suffix='_fold_{}'.format(fold_id))\n",
    "    LOGGER.info('Start pipeline fit and transform on train')\n",
    "    pipeline.clean_cache()\n",
    "    pipeline.fit_transform(train_pipe_input)\n",
    "    pipeline.clean_cache()\n",
    "\n",
    "    pipeline = PIPELINES[pipeline_name](config=config, train_mode=False, suffix='_fold_{}'.format(fold_id))\n",
    "    LOGGER.info('Start pipeline transform on valid')\n",
    "    pipeline.clean_cache()\n",
    "    output_valid = pipeline.transform(valid_pipe_input)\n",
    "    pipeline.clean_cache()\n",
    "\n",
    "    y_pred = output_valid['y_pred']\n",
    "    y_true = read_masks(valid_data_split[cfg.Y_COLUMNS[0]].values)\n",
    "\n",
    "    iou = intersection_over_union(y_true, y_pred)\n",
    "    iout = intersection_over_union_thresholds(y_true, y_pred)\n",
    "\n",
    "    return iou, iout, y_pred, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "#            src/utils.py           #\n",
    "#####################################\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from itertools import chain\n",
    "from collections import Iterable\n",
    "\n",
    "from deepsense import neptune\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from attrdict import AttrDict\n",
    "from tqdm import tqdm\n",
    "from pycocotools import mask as cocomask\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from steppy.base import BaseTransformer\n",
    "import yaml\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "\n",
    "NEPTUNE_CONFIG_PATH = str(pathlib.Path(__file__).resolve().parents[1] / 'configs' / 'neptune.yaml')\n",
    "\n",
    "\n",
    "# Alex Martelli's 'Borg'\n",
    "# http://python-3-patterns-idioms-test.readthedocs.io/en/latest/Singleton.html\n",
    "class _Borg:\n",
    "    _shared_state = {}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__dict__ = self._shared_state\n",
    "\n",
    "\n",
    "class NeptuneContext(_Borg):\n",
    "    def __init__(self, fallback_file=NEPTUNE_CONFIG_PATH):\n",
    "        _Borg.__init__(self)\n",
    "\n",
    "        self.ctx = neptune.Context()\n",
    "        self.fallback_file = fallback_file\n",
    "        self.params = self._read_params()\n",
    "        self.numeric_channel = neptune.ChannelType.NUMERIC\n",
    "        self.image_channel = neptune.ChannelType.IMAGE\n",
    "        self.text_channel = neptune.ChannelType.TEXT\n",
    "\n",
    "    def channel_send(self, *args, **kwargs):\n",
    "        self.ctx.channel_send(*args, **kwargs)\n",
    "\n",
    "    def _read_params(self):\n",
    "        if self.ctx.params.__class__.__name__ == 'OfflineContextParams':\n",
    "            params = self._read_yaml().parameters\n",
    "        else:\n",
    "            params = self.ctx.params\n",
    "        return params\n",
    "\n",
    "    def _read_yaml(self):\n",
    "        with open(self.fallback_file) as f:\n",
    "            config = yaml.load(f)\n",
    "        return AttrDict(config)\n",
    "\n",
    "\n",
    "def init_logger():\n",
    "    logger = logging.getLogger('salt-detection')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    message_format = logging.Formatter(fmt='%(asctime)s %(name)s >>> %(message)s',\n",
    "                                       datefmt='%Y-%m-%d %H-%M-%S')\n",
    "\n",
    "    # console handler for validation info\n",
    "    ch_va = logging.StreamHandler(sys.stdout)\n",
    "    ch_va.setLevel(logging.INFO)\n",
    "\n",
    "    ch_va.setFormatter(fmt=message_format)\n",
    "\n",
    "    # add the handlers to the logger\n",
    "    logger.addHandler(ch_va)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def get_logger():\n",
    "    return logging.getLogger('salt-detection')\n",
    "\n",
    "\n",
    "def decompose(labeled):\n",
    "    nr_true = labeled.max()\n",
    "    masks = []\n",
    "    for i in range(1, nr_true + 1):\n",
    "        msk = labeled.copy()\n",
    "        msk[msk != i] = 0.\n",
    "        msk[msk == i] = 255.\n",
    "        masks.append(msk)\n",
    "\n",
    "    if not masks:\n",
    "        return [labeled]\n",
    "    else:\n",
    "        return masks\n",
    "\n",
    "\n",
    "def create_submission(meta, predictions):\n",
    "    output = []\n",
    "    for image_id, mask in zip(meta['id'].values, predictions):\n",
    "        rle_encoded = ' '.join(str(rle) for rle in run_length_encoding(mask))\n",
    "        output.append([image_id, rle_encoded])\n",
    "\n",
    "    submission = pd.DataFrame(output, columns=['id', 'rle_mask']).astype(str)\n",
    "    return submission\n",
    "\n",
    "\n",
    "def encode_rle(predictions):\n",
    "    return [run_length_encoding(mask) for mask in predictions]\n",
    "\n",
    "\n",
    "def read_masks(masks_filepaths):\n",
    "    masks = []\n",
    "    for mask_filepath in tqdm(masks_filepaths):\n",
    "        mask = Image.open(mask_filepath)\n",
    "        mask = np.asarray(mask.convert('L').point(lambda x: 0 if x < 128 else 1)).astype(np.uint8)\n",
    "        masks.append(mask)\n",
    "    return masks\n",
    "\n",
    "\n",
    "def read_images(filepaths):\n",
    "    images = []\n",
    "    for filepath in filepaths:\n",
    "        image = np.array(Image.open(filepath))\n",
    "        images.append(image)\n",
    "    return images\n",
    "\n",
    "\n",
    "def run_length_encoding(x):\n",
    "    # https://www.kaggle.com/c/data-science-bowl-2018/discussion/48561#\n",
    "    bs = np.where(x.T.flatten())[0]\n",
    "\n",
    "    rle = []\n",
    "    prev = -2\n",
    "    for b in bs:\n",
    "        if (b > prev + 1): rle.extend((b + 1, 0))\n",
    "        rle[-1] += 1\n",
    "        prev = b\n",
    "\n",
    "    if len(rle) != 0 and rle[-1] + rle[-2] == x.size:\n",
    "        rle[-2] = rle[-2] - 1\n",
    "\n",
    "    return rle\n",
    "\n",
    "\n",
    "def run_length_decoding(mask_rle, shape):\n",
    "    \"\"\"\n",
    "    Based on https://www.kaggle.com/msl23518/visualize-the-stage1-test-solution and modified\n",
    "    Args:\n",
    "        mask_rle: run-length as string formatted (start length)\n",
    "        shape: (height, width) of array to return\n",
    "\n",
    "    Returns:\n",
    "        numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    \"\"\"\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[1] * shape[0], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape((shape[1], shape[0])).T\n",
    "\n",
    "\n",
    "def generate_metadata(train_images_dir, test_images_dir, depths_filepath):\n",
    "    \n",
    "    depths = pd.read_csv(depths_filepath) # pandas读入深度数据文件\n",
    "\n",
    "    metadata = {} # 字典\n",
    "    for filename in tqdm(os.listdir(os.path.join(train_images_dir, 'images'))):\n",
    "        image_filepath = os.path.join(train_images_dir, 'images', filename)  # 生成每张图片的文件路径\n",
    "        mask_filepath = os.path.join(train_images_dir, 'masks', filename)    # 生成每张图片的标注图片的文件路径\n",
    "        image_id = filename.split('.')[0]  # 截取 xxx.png 的 xxx 作为图片ID\n",
    "        depth = depths[depths['id'] == image_id]['z'].values[0]  # 提取该图片的深度数值(标签为 'z'， 单位：英尺)\n",
    "        \n",
    "        # 思路不错    先用个大字典存储原始训练和测试数据信息    根据is_train标志区分每张图片是用于训练还是测试\n",
    "        metadata.setdefault('file_path_image', []).append(image_filepath)   # 初始化为空列表 然后添加数据 高效技巧！！！！！！！！！！！\n",
    "        metadata.setdefault('file_path_mask', []).append(mask_filepath)\n",
    "        metadata.setdefault('is_train', []).append(1)\n",
    "        metadata.setdefault('id', []).append(image_id)\n",
    "        metadata.setdefault('z', []).append(depth)\n",
    "\n",
    "    for filename in tqdm(os.listdir(os.path.join(test_images_dir, 'images'))):\n",
    "        image_filepath = os.path.join(test_images_dir, 'images', filename)\n",
    "        image_id = filename.split('.')[0]\n",
    "        depth = depths[depths['id'] == image_id]['z'].values[0]\n",
    "\n",
    "        metadata.setdefault('file_path_image', []).append(image_filepath)\n",
    "        metadata.setdefault('file_path_mask', []).append(None)\n",
    "        metadata.setdefault('is_train', []).append(0)\n",
    "        metadata.setdefault('id', []).append(image_id)\n",
    "        metadata.setdefault('z', []).append(depth)\n",
    "\n",
    "    return pd.DataFrame(metadata)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(X, theta=1.0, axis=None):\n",
    "    \"\"\"\n",
    "    https://nolanbconaway.github.io/blog/2017/softmax-numpy\n",
    "    Compute the softmax of each element along an axis of X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ND-Array. Probably should be floats.\n",
    "    theta (optional): float parameter, used as a multiplier\n",
    "        prior to exponentiation. Default = 1.0\n",
    "    axis (optional): axis to compute values along. Default is the\n",
    "        first non-singleton axis.\n",
    "\n",
    "    Returns an array the same size as X. The result will sum to 1\n",
    "    along the specified axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # make X at least 2d\n",
    "    y = np.atleast_2d(X)\n",
    "\n",
    "    # find axis\n",
    "    if axis is None:\n",
    "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
    "\n",
    "    # multiply y against the theta parameter,\n",
    "    y = y * float(theta)\n",
    "\n",
    "    # subtract the max for numerical stability\n",
    "    y = y - np.expand_dims(np.max(y, axis=axis), axis)\n",
    "\n",
    "    # exponentiate y\n",
    "    y = np.exp(y)\n",
    "\n",
    "    # take the sum along the specified axis\n",
    "    ax_sum = np.expand_dims(np.sum(y, axis=axis), axis)\n",
    "\n",
    "    # finally: divide elementwise\n",
    "    p = y / ax_sum\n",
    "\n",
    "    # flatten if X was 1D\n",
    "    if len(X.shape) == 1: p = p.flatten()\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "def from_pil(*images):\n",
    "    images = [np.array(image) for image in images]\n",
    "    if len(images) == 1:\n",
    "        return images[0]\n",
    "    else:\n",
    "        return images\n",
    "\n",
    "\n",
    "def to_pil(*images):\n",
    "    images = [Image.fromarray((image).astype(np.uint8)) for image in images]\n",
    "    if len(images) == 1:\n",
    "        return images[0]\n",
    "    else:\n",
    "        return images\n",
    "\n",
    "\n",
    "def make_apply_transformer(func, output_name='output', apply_on=None):\n",
    "    class StaticApplyTransformer(BaseTransformer):\n",
    "        def transform(self, *args, **kwargs):\n",
    "            self.check_input(*args, **kwargs)\n",
    "\n",
    "            if not apply_on:\n",
    "                iterator = zip(*args, *kwargs.values())\n",
    "            else:\n",
    "                iterator = zip(*args, *[kwargs[key] for key in apply_on])\n",
    "\n",
    "            output = []\n",
    "            for func_args in tqdm(iterator, total=self.get_arg_length(*args, **kwargs)):\n",
    "                output.append(func(*func_args))\n",
    "            return {output_name: output}\n",
    "\n",
    "        @staticmethod\n",
    "        def check_input(*args, **kwargs):\n",
    "            if len(args) and len(kwargs) == 0:\n",
    "                raise Exception('Input must not be empty')\n",
    "\n",
    "            arg_length = None\n",
    "            for arg in chain(args, kwargs.values()):\n",
    "                if not isinstance(arg, Iterable):\n",
    "                    raise Exception('All inputs must be iterable')\n",
    "                arg_length_loc = None\n",
    "                try:\n",
    "                    arg_length_loc = len(arg)\n",
    "                except:\n",
    "                    pass\n",
    "                if arg_length_loc is not None:\n",
    "                    if arg_length is None:\n",
    "                        arg_length = arg_length_loc\n",
    "                    elif arg_length_loc != arg_length:\n",
    "                        raise Exception('All inputs must be the same length')\n",
    "\n",
    "        @staticmethod\n",
    "        def get_arg_length(*args, **kwargs):\n",
    "            arg_length = None\n",
    "            for arg in chain(args, kwargs.values()):\n",
    "                if arg_length is None:\n",
    "                    try:\n",
    "                        arg_length = len(arg)\n",
    "                    except:\n",
    "                        pass\n",
    "                if arg_length is not None:\n",
    "                    return arg_length\n",
    "\n",
    "    return StaticApplyTransformer()\n",
    "\n",
    "\n",
    "def rle_from_binary(prediction):\n",
    "    prediction = np.asfortranarray(prediction)\n",
    "    return cocomask.encode(prediction)\n",
    "\n",
    "\n",
    "def binary_from_rle(rle):\n",
    "    return cocomask.decode(rle)\n",
    "\n",
    "\n",
    "def get_segmentations(labeled):\n",
    "    nr_true = labeled.max()\n",
    "    segmentations = []\n",
    "    for i in range(1, nr_true + 1):\n",
    "        msk = labeled == i\n",
    "        segmentation = rle_from_binary(msk.astype('uint8'))\n",
    "        segmentation['counts'] = segmentation['counts'].decode(\"UTF-8\")\n",
    "        segmentations.append(segmentation)\n",
    "    return segmentations\n",
    "\n",
    "\n",
    "def get_crop_pad_sequence(vertical, horizontal):\n",
    "    top = int(vertical / 2)\n",
    "    bottom = vertical - top\n",
    "    right = int(horizontal / 2)\n",
    "    left = horizontal - right\n",
    "    return (top, right, bottom, left)\n",
    "\n",
    "\n",
    "def get_list_of_image_predictions(batch_predictions):\n",
    "    image_predictions = []\n",
    "    for batch_pred in batch_predictions:\n",
    "        image_predictions.extend(list(batch_pred))\n",
    "    return image_predictions\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "class ImgAug:\n",
    "    def __init__(self, augmenters):\n",
    "        if not isinstance(augmenters, list):\n",
    "            augmenters = [augmenters]\n",
    "        self.augmenters = augmenters\n",
    "        self.seq_det = None\n",
    "\n",
    "    def _pre_call_hook(self):\n",
    "        seq = iaa.Sequential(self.augmenters)\n",
    "        seq = reseed(seq, deterministic=True)\n",
    "        self.seq_det = seq\n",
    "\n",
    "    def transform(self, *images):\n",
    "        images = [self.seq_det.augment_image(image) for image in images]\n",
    "        if len(images) == 1:\n",
    "            return images[0]\n",
    "        else:\n",
    "            return images\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        self._pre_call_hook()\n",
    "        return self.transform(*args)\n",
    "\n",
    "\n",
    "def get_seed():\n",
    "    seed = int(time.time()) + int(os.getpid())\n",
    "    return seed\n",
    "\n",
    "\n",
    "def reseed(augmenter, deterministic=True):\n",
    "    augmenter.random_state = ia.new_random_state(get_seed())\n",
    "    if deterministic:\n",
    "        augmenter.deterministic = True\n",
    "\n",
    "    for lists in augmenter.get_children_lists():\n",
    "        for aug in lists:\n",
    "            aug = reseed(aug, deterministic=True)\n",
    "    return augmenter\n",
    "\n",
    "\n",
    "# BaseCrossValidator 来自scikit learn\n",
    "class KFoldBySortedValue(BaseCrossValidator): \n",
    "    def __init__(self, n_splits=3, shuffle=False, random_state=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.shuffle = shuffle\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _iter_test_indices(self, X, y=None, groups=None):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "        sorted_idx_vals = sorted(zip(indices, X), key=lambda x: x[1])\n",
    "        indices = [idx for idx, val in sorted_idx_vals]\n",
    "\n",
    "        for split_start in range(self.n_splits):\n",
    "            split_indeces = indices[split_start::self.n_splits]\n",
    "            yield split_indeces\n",
    "\n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "\n",
    "def plot_list(images=[], labels=[]):\n",
    "    n_img = len(images)\n",
    "    n_lab = len(labels)\n",
    "    n = n_lab + n_img\n",
    "    fig, axs = plt.subplots(1, n, figsize=(16, 12))\n",
    "    for i, image in enumerate(images):\n",
    "        axs[i].imshow(image)\n",
    "        axs[i].set_xticks([])\n",
    "        axs[i].set_yticks([])\n",
    "    for j, label in enumerate(labels):\n",
    "        axs[n_img + j].imshow(label, cmap='nipy_spectral')\n",
    "        axs[n_img + j].set_xticks([])\n",
    "        axs[n_img + j].set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def clean_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycocotools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-217c22f6e8fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclick\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipelineManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpipeline_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/ml/YOLO_competition/src/pipeline_manager.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mintersection_over_union\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintersection_over_union_thresholds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline_config\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpipelines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPIPELINES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/ml/YOLO_competition/src/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcocomask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_segmentations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycocotools'"
     ]
    }
   ],
   "source": [
    "import click\n",
    "from src.pipeline_manager import PipelineManager\n",
    "\n",
    "pipeline_manager = PipelineManager()\n",
    "\n",
    "\n",
    "@click.group()\n",
    "def main():\n",
    "    pass\n",
    "\n",
    "\n",
    "@main.command()\n",
    "def prepare_metadata():\n",
    "    pipeline_manager.prepare_metadata()\n",
    "\n",
    "\n",
    "@main.command()\n",
    "@click.option('-p', '--pipeline_name', help='pipeline to be trained', required=True)\n",
    "@click.option('-d', '--dev_mode', help='if true only a small sample of data will be used', is_flag=True, required=False)\n",
    "def train(pipeline_name, dev_mode):\n",
    "    pipeline_manager.train(pipeline_name, dev_mode)\n",
    "\n",
    "\n",
    "@main.command()\n",
    "@click.option('-p', '--pipeline_name', help='pipeline to be trained', required=True)\n",
    "@click.option('-d', '--dev_mode', help='if true only a small sample of data will be used', is_flag=True, required=False)\n",
    "def evaluate(pipeline_name, dev_mode):\n",
    "    pipeline_manager.evaluate(pipeline_name, dev_mode)\n",
    "\n",
    "\n",
    "@main.command()\n",
    "@click.option('-p', '--pipeline_name', help='pipeline to be trained', required=True)\n",
    "@click.option('-s', '--submit_predictions', help='submit predictions if true', is_flag=True, required=False)\n",
    "@click.option('-d', '--dev_mode', help='if true only a small sample of data will be used', is_flag=True, required=False)\n",
    "def predict(pipeline_name, submit_predictions, dev_mode):\n",
    "    pipeline_manager.predict(pipeline_name, submit_predictions, dev_mode)\n",
    "\n",
    "\n",
    "@main.command()\n",
    "@click.option('-p', '--pipeline_name', help='pipeline to be trained', required=True)\n",
    "@click.option('-s', '--submit_predictions', help='submit predictions if true', is_flag=True, required=False)\n",
    "@click.option('-d', '--dev_mode', help='if true only a small sample of data will be used', is_flag=True, required=False)\n",
    "def train_evaluate_predict(pipeline_name, submit_predictions, dev_mode):\n",
    "    pipeline_manager.train(pipeline_name, dev_mode)\n",
    "    pipeline_manager.evaluate(pipeline_name, dev_mode)\n",
    "    pipeline_manager.predict(pipeline_name, submit_predictions, dev_mode)\n",
    "\n",
    "\n",
    "@main.command()\n",
    "@click.option('-p', '--pipeline_name', help='pipeline to be trained', required=True)\n",
    "@click.option('-d', '--dev_mode', help='if true only a small sample of data will be used', is_flag=True, required=False)\n",
    "def train_evaluate(pipeline_name, dev_mode):\n",
    "    pipeline_manager.train(pipeline_name, dev_mode)\n",
    "    pipeline_manager.evaluate(pipeline_name, dev_mode)\n",
    "\n",
    "\n",
    "@main.command()\n",
    "@click.option('-p', '--pipeline_name', help='pipeline to be trained', required=True)\n",
    "@click.option('-s', '--submit_predictions', help='submit predictions if true', is_flag=True, required=False)\n",
    "@click.option('-d', '--dev_mode', help='if true only a small sample of data will be used', is_flag=True, required=False)\n",
    "def evaluate_predict(pipeline_name, submit_predictions, dev_mode):\n",
    "    pipeline_manager.evaluate(pipeline_name, dev_mode)\n",
    "    pipeline_manager.predict(pipeline_name, submit_predictions, dev_mode)\n",
    "\n",
    "\n",
    "@main.command()\n",
    "@click.option('-p', '--pipeline_name', help='pipeline to be trained', required=True)\n",
    "@click.option('-d', '--dev_mode', help='if true only a small sample of data will be used', is_flag=True, required=False)\n",
    "def train_evaluate_cv(pipeline_name, dev_mode):\n",
    "    pipeline_manager.train_evaluate_cv(pipeline_name, dev_mode)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
